# Concurrent URL Downloader
# Demonstrates async/spawn for parallel HTTP requests
# Run with: cargo run -- run examples/async_downloader.forma

# Fetch a single URL and return success status
as f fetch_url(url: Str) -> Bool
    print("Fetching: " + url)
    m http_get(url)
        Ok(_) ->
            print("Done: " + url)
            true
        Err(e) ->
            print("Failed: " + url + " - " + e)
            false

f main() -> Int
    print("=== FORMA URL Downloader ===")
    print("")

    # List of URLs to fetch
    urls := vec_new()
    urls = vec_push(urls, "https://httpbin.org/get")
    urls = vec_push(urls, "https://httpbin.org/ip")
    urls = vec_push(urls, "https://httpbin.org/user-agent")

    url_count := vec_len(urls)
    print("Downloading " + int_to_str(url_count) + " URLs in parallel...")
    print("")

    start := time_now_ms()

    # Spawn tasks for each URL in parallel
    tasks := vec_new()
    for url in urls
        tasks = vec_push(tasks, sp fetch_url(url))

    # Wait for all tasks to complete
    results := await_all(tasks)

    # Count successes and failures
    successes := 0
    failures := 0
    for result in results
        if result then successes := successes + 1 else failures := failures + 1

    elapsed := time_now_ms() - start
    print("")
    print("All downloads complete in " + int_to_str(elapsed) + "ms")
    print("")
    print("Summary:")
    print("  Successful: " + int_to_str(successes))
    print("  Failed: " + int_to_str(failures))
    print("  Total: " + int_to_str(url_count))

    0
