# Test the FORMA bootstrap lexer
# Expected output: All tests pass, final result: 1

# ============================================================
# Test Token definitions
# ============================================================

f test_token_kinds() -> Bool
    # Test that token kind constants are distinct
    TK_F() != TK_S() &&
    TK_IF() != TK_THEN() &&
    TK_PLUS() != TK_MINUS() &&
    TK_EOF() != TK_ERROR()

f test_span_creation() -> Bool
    span := span_new(0, 10, 1, 1)
    span.start == 0 && span.end == 10 && span.line == 1 && span.column == 1

f test_span_len() -> Bool
    span := span_new(5, 15, 2, 3)
    span_len(span) == 10

f test_token_creation() -> Bool
    span := span_new(0, 5, 1, 1)
    tok := token_new(TK_IDENT(), span, "hello")
    tok.kind == TK_IDENT() && tok.lexeme == "hello"

f test_keyword_lookup() -> Bool
    # Test keyword detection
    m keyword_lookup("f")
        Some(k) -> k == TK_F()
        None -> false
    &&
    m keyword_lookup("if")
        Some(k) -> k == TK_IF()
        None -> false
    &&
    m keyword_lookup("true")
        Some(k) -> k == TK_TRUE()
        None -> false
    &&
    m keyword_lookup("notakeyword")
        Some(_) -> false
        None -> true

# ============================================================
# Test Scanner basics
# ============================================================

f test_scanner_creation() -> Bool
    s := scanner_new("hello world")
    s.pos == 0 && s.line == 1 && s.column == 1

f test_scanner_peek() -> Bool
    s := scanner_new("abc")
    m scanner_peek(s)
        Some(c) -> c == 'a'
        None -> false

f test_scanner_advance() -> Bool
    s := scanner_new("abc")
    (s2, opt_c) := scanner_advance(s)
    m opt_c
        Some(c) -> c == 'a' && s2.pos == 1
        None -> false

f test_scanner_at_end() -> Bool
    s1 := scanner_new("")
    s2 := scanner_new("x")
    scanner_at_end(s1) && !scanner_at_end(s2)

# ============================================================
# Test Character classification
# ============================================================

f test_char_classification() -> Bool
    is_digit('5') &&
    !is_digit('a') &&
    is_alpha('x') &&
    !is_alpha('9') &&
    is_ident_start('_') &&
    is_ident_start('a') &&
    !is_ident_start('9') &&
    is_ident_continue('_') &&
    is_ident_continue('a') &&
    is_ident_continue('5')

# ============================================================
# Test Full tokenization
# ============================================================

f test_scan_simple_tokens() -> Bool
    (s, tokens) := scan_all("+ - * /")
    # Should have: PLUS MINUS STAR SLASH EOF
    vec_len(tokens) >= 5 &&
    tokens[0].kind == TK_PLUS() &&
    tokens[1].kind == TK_MINUS() &&
    tokens[2].kind == TK_STAR() &&
    tokens[3].kind == TK_SLASH()

f test_scan_two_char_tokens() -> Bool
    (s, tokens) := scan_all("== != -> :=")
    vec_len(tokens) >= 4 &&
    tokens[0].kind == TK_EQEQ() &&
    tokens[1].kind == TK_BANGEQ() &&
    tokens[2].kind == TK_ARROW() &&
    tokens[3].kind == TK_COLONEQ()

f test_scan_keywords() -> Bool
    (s, tokens) := scan_all("f if then else")
    vec_len(tokens) >= 4 &&
    tokens[0].kind == TK_F() &&
    tokens[1].kind == TK_IF() &&
    tokens[2].kind == TK_THEN() &&
    tokens[3].kind == TK_ELSE()

f test_scan_identifiers() -> Bool
    (s, tokens) := scan_all("foo bar_baz x123")
    vec_len(tokens) >= 3 &&
    tokens[0].kind == TK_IDENT() &&
    tokens[0].str_val == "foo" &&
    tokens[1].kind == TK_IDENT() &&
    tokens[2].kind == TK_IDENT()

f test_scan_integers() -> Bool
    (s, tokens) := scan_all("42 123 0")
    vec_len(tokens) >= 3 &&
    tokens[0].kind == TK_INT() &&
    tokens[0].int_val == 42 &&
    tokens[1].kind == TK_INT() &&
    tokens[1].int_val == 123

f test_scan_string() -> Bool
    (s, tokens) := scan_all("\"hello world\"")
    vec_len(tokens) >= 1 &&
    tokens[0].kind == TK_STRING() &&
    tokens[0].str_val == "hello world"

f test_scan_char() -> Bool
    (s, tokens) := scan_all("'a' 'b'")
    vec_len(tokens) >= 2 &&
    tokens[0].kind == TK_CHAR() &&
    tokens[0].char_val == 'a' &&
    tokens[1].kind == TK_CHAR()

f test_scan_newline_indent() -> Bool
    (s, tokens) := scan_all("f foo\n    x")
    # Should have: F IDENT NEWLINE INDENT IDENT EOF
    has_indent := false
    for tok in tokens
        if tok.kind == TK_INDENT() then has_indent = true else has_indent = has_indent
    has_indent

f test_scan_dedent() -> Bool
    (s, tokens) := scan_all("f foo\n    x\ny")
    # Should have dedent when going back to column 0
    has_dedent := false
    for tok in tokens
        if tok.kind == TK_DEDENT() then has_dedent = true else has_dedent = has_dedent
    has_dedent

# ============================================================
# Run all tests
# ============================================================

f run_all_tests() -> Int
    passed := 0
    total := 17

    # Token tests
    if test_token_kinds() then passed = passed + 1 else print("FAIL: test_token_kinds")
    if test_span_creation() then passed = passed + 1 else print("FAIL: test_span_creation")
    if test_span_len() then passed = passed + 1 else print("FAIL: test_span_len")
    if test_token_creation() then passed = passed + 1 else print("FAIL: test_token_creation")
    if test_keyword_lookup() then passed = passed + 1 else print("FAIL: test_keyword_lookup")

    # Scanner basics
    if test_scanner_creation() then passed = passed + 1 else print("FAIL: test_scanner_creation")
    if test_scanner_peek() then passed = passed + 1 else print("FAIL: test_scanner_peek")
    if test_scanner_advance() then passed = passed + 1 else print("FAIL: test_scanner_advance")
    if test_scanner_at_end() then passed = passed + 1 else print("FAIL: test_scanner_at_end")

    # Character classification
    if test_char_classification() then passed = passed + 1 else print("FAIL: test_char_classification")

    # Full tokenization
    if test_scan_simple_tokens() then passed = passed + 1 else print("FAIL: test_scan_simple_tokens")
    if test_scan_two_char_tokens() then passed = passed + 1 else print("FAIL: test_scan_two_char_tokens")
    if test_scan_keywords() then passed = passed + 1 else print("FAIL: test_scan_keywords")
    if test_scan_identifiers() then passed = passed + 1 else print("FAIL: test_scan_identifiers")
    if test_scan_integers() then passed = passed + 1 else print("FAIL: test_scan_integers")
    if test_scan_string() then passed = passed + 1 else print("FAIL: test_scan_string")
    if test_scan_char() then passed = passed + 1 else print("FAIL: test_scan_char")

    print("Lexer tests passed:")
    print(passed)
    print(str_concat("of ", int_to_str(total)))

    if passed == total then 1 else 0

f main() -> Int = run_all_tests()
