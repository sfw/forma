# FORMA Bootstrap Compiler - Token Definitions
# Defines all tokens that can appear in FORMA source code

# ============================================================
# Token Kind enumeration
# Using integers to represent token kinds since FORMA enums
# with data require more complex pattern matching
# ============================================================

# Token kind constants
# Keywords (single character) - 1-10
s TokenKind
    # Store the discriminant
    kind: Int
    # For tokens with associated data, store separately
    int_value: Int
    float_value: Float
    str_value: Str
    char_value: Char

# Token kind constants
# Keywords (single character)
f TK_F() -> Int = 1          # function
f TK_S() -> Int = 2          # struct
f TK_E() -> Int = 3          # enum
f TK_T() -> Int = 4          # trait
f TK_I() -> Int = 5          # impl
f TK_M() -> Int = 6          # match

# Keywords (multi-character) - 10-40
f TK_IF() -> Int = 10
f TK_THEN() -> Int = 11
f TK_ELSE() -> Int = 12
f TK_FOR() -> Int = 13
f TK_IN() -> Int = 14
f TK_WH() -> Int = 15        # while
f TK_LP() -> Int = 16        # loop
f TK_BR() -> Int = 17        # break
f TK_CT() -> Int = 18        # continue
f TK_RET() -> Int = 19       # return
f TK_AS() -> Int = 20        # async
f TK_AW() -> Int = 21        # await
f TK_US() -> Int = 22        # use
f TK_MD() -> Int = 23        # module
f TK_PUB() -> Int = 24       # public
f TK_MUT() -> Int = 25       # mutable
f TK_MV() -> Int = 26        # move
f TK_UN() -> Int = 27        # unsafe
f TK_TYPE() -> Int = 28      # type alias
f TK_WHERE() -> Int = 29     # where clause

# Boolean/None literals - 40-45
f TK_TRUE() -> Int = 40
f TK_FALSE() -> Int = 41
f TK_NONE() -> Int = 42

# Built-in type constructors - 45-50
f TK_SOME() -> Int = 45
f TK_OK() -> Int = 46
f TK_ERR() -> Int = 47

# Arithmetic operators - 50-60
f TK_PLUS() -> Int = 50      # +
f TK_MINUS() -> Int = 51     # -
f TK_STAR() -> Int = 52      # *
f TK_SLASH() -> Int = 53     # /
f TK_PERCENT() -> Int = 54   # %

# Comparison operators - 60-70
f TK_EQEQ() -> Int = 60      # ==
f TK_BANGEQ() -> Int = 61    # !=
f TK_LT() -> Int = 62        # <
f TK_LTEQ() -> Int = 63      # <=
f TK_GT() -> Int = 64        # >
f TK_GTEQ() -> Int = 65      # >=

# Logical operators - 70-75
f TK_AMPAMP() -> Int = 70    # &&
f TK_PIPEPIPE() -> Int = 71  # ||
f TK_BANG() -> Int = 72      # !

# Bitwise operators - 75-80
f TK_AMP() -> Int = 75       # &
f TK_PIPE() -> Int = 76      # |
f TK_CARET() -> Int = 77     # ^
f TK_LTLT() -> Int = 78      # <<
f TK_GTGT() -> Int = 79      # >>

# Assignment operators - 80-90
f TK_EQ() -> Int = 80        # =
f TK_COLONEQ() -> Int = 81   # :=
f TK_PLUSEQ() -> Int = 82    # +=
f TK_MINUSEQ() -> Int = 83   # -=
f TK_STAREQ() -> Int = 84    # *=
f TK_SLASHEQ() -> Int = 85   # /=

# Special operators - 90-100
f TK_QUESTION() -> Int = 90  # ?
f TK_QUESTIONQUESTION() -> Int = 91  # ??
f TK_ARROW() -> Int = 92     # ->
f TK_FATARROW() -> Int = 93  # =>
f TK_DOTDOT() -> Int = 94    # ..
f TK_DOTDOTEQ() -> Int = 95  # ..=
f TK_COLONCOLON() -> Int = 96 # ::
f TK_DOT() -> Int = 97       # .
f TK_COMMA() -> Int = 98     # ,
f TK_AT() -> Int = 99        # @

# Delimiters - 100-110
f TK_LPAREN() -> Int = 100   # (
f TK_RPAREN() -> Int = 101   # )
f TK_LBRACKET() -> Int = 102 # [
f TK_RBRACKET() -> Int = 103 # ]
f TK_LBRACE() -> Int = 104   # {
f TK_RBRACE() -> Int = 105   # }
f TK_COLON() -> Int = 106    # :
f TK_SEMICOLON() -> Int = 107 # ;

# Literals - 110-120 (with associated data)
f TK_INT() -> Int = 110
f TK_FLOAT() -> Int = 111
f TK_STRING() -> Int = 112
f TK_CHAR() -> Int = 113

# Identifiers - 120
f TK_IDENT() -> Int = 120

# Indentation tokens - 130-135
f TK_NEWLINE() -> Int = 130
f TK_INDENT() -> Int = 131
f TK_DEDENT() -> Int = 132

# Special - 140-145
f TK_EOF() -> Int = 140
f TK_ERROR() -> Int = 141

# ============================================================
# Span - Source location information
# ============================================================

s Span
    start: Int
    end: Int
    line: Int
    column: Int

f span_new(start: Int, end: Int, line: Int, column: Int) -> Span = Span { start: start, end: end, line: line, column: column }

f span_len(span: Span) -> Int = span.end - span.start

f span_merge(a: Span, b: Span) -> Span
    start := if a.start < b.start then a.start else b.start
    end := if a.end > b.end then a.end else b.end
    line := if a.line < b.line then a.line else b.line
    col := if a.line <= b.line then a.column else b.column
    Span { start: start, end: end, line: line, column: col }

# ============================================================
# Token - A token with its location and value
# ============================================================

s Token
    kind: Int           # TokenKind constant
    span: Span
    lexeme: Str
    # For literals with values
    int_val: Int
    float_val: Float
    str_val: Str
    char_val: Char

f token_new(kind: Int, span: Span, lexeme: Str) -> Token = Token { kind: kind, span: span, lexeme: lexeme, int_val: 0, float_val: 0.0, str_val: "", char_val: ' ' }

f token_new_int(kind: Int, span: Span, lexeme: Str, value: Int) -> Token = Token { kind: kind, span: span, lexeme: lexeme, int_val: value, float_val: 0.0, str_val: "", char_val: ' ' }

f token_new_float(kind: Int, span: Span, lexeme: Str, value: Float) -> Token = Token { kind: kind, span: span, lexeme: lexeme, int_val: 0, float_val: value, str_val: "", char_val: ' ' }

f token_new_str(kind: Int, span: Span, lexeme: Str, value: Str) -> Token = Token { kind: kind, span: span, lexeme: lexeme, int_val: 0, float_val: 0.0, str_val: value, char_val: ' ' }

f token_new_char(kind: Int, span: Span, lexeme: Str, value: Char) -> Token = Token { kind: kind, span: span, lexeme: lexeme, int_val: 0, float_val: 0.0, str_val: "", char_val: value }

f token_is(tok: Token, kind: Int) -> Bool
    tok.kind == kind

# ============================================================
# Keyword lookup
# ============================================================

f keyword_lookup(s: Str) -> Option[Int]
    # Single-character keywords
    if s == "f" then ret Some(TK_F()) else 0
    if s == "s" then ret Some(TK_S()) else 0
    if s == "e" then ret Some(TK_E()) else 0
    if s == "t" then ret Some(TK_T()) else 0
    if s == "i" then ret Some(TK_I()) else 0
    if s == "m" then ret Some(TK_M()) else 0
    # Multi-character keywords
    if s == "if" then ret Some(TK_IF()) else 0
    if s == "then" then ret Some(TK_THEN()) else 0
    if s == "else" then ret Some(TK_ELSE()) else 0
    if s == "for" then ret Some(TK_FOR()) else 0
    if s == "in" then ret Some(TK_IN()) else 0
    if s == "wh" then ret Some(TK_WH()) else 0
    if s == "lp" then ret Some(TK_LP()) else 0
    if s == "br" then ret Some(TK_BR()) else 0
    if s == "ct" then ret Some(TK_CT()) else 0
    if s == "ret" then ret Some(TK_RET()) else 0
    if s == "as" then ret Some(TK_AS()) else 0
    if s == "aw" then ret Some(TK_AW()) else 0
    if s == "us" then ret Some(TK_US()) else 0
    if s == "md" then ret Some(TK_MD()) else 0
    if s == "pub" then ret Some(TK_PUB()) else 0
    if s == "mut" then ret Some(TK_MUT()) else 0
    if s == "mv" then ret Some(TK_MV()) else 0
    if s == "un" then ret Some(TK_UN()) else 0
    if s == "type" then ret Some(TK_TYPE()) else 0
    if s == "where" then ret Some(TK_WHERE()) else 0
    # Boolean literals
    if s == "T" then ret Some(TK_TRUE()) else 0
    if s == "F" then ret Some(TK_FALSE()) else 0
    if s == "true" then ret Some(TK_TRUE()) else 0
    if s == "false" then ret Some(TK_FALSE()) else 0
    # None literal
    if s == "N" then ret Some(TK_NONE()) else 0
    if s == "none" then ret Some(TK_NONE()) else 0
    # Built-in constructors
    if s == "Some" then ret Some(TK_SOME()) else 0
    if s == "Ok" then ret Some(TK_OK()) else 0
    if s == "Err" then ret Some(TK_ERR()) else 0
    if s == "ok" then ret Some(TK_OK()) else 0
    if s == "err" then ret Some(TK_ERR()) else 0
    None

# ============================================================
# Token kind name (for debugging)
# ============================================================

f token_kind_name(kind: Int) -> Str
    if kind == TK_F() then ret "F" else 0
    if kind == TK_S() then ret "S" else 0
    if kind == TK_E() then ret "E" else 0
    if kind == TK_T() then ret "T" else 0
    if kind == TK_I() then ret "I" else 0
    if kind == TK_M() then ret "M" else 0
    if kind == TK_IF() then ret "IF" else 0
    if kind == TK_THEN() then ret "THEN" else 0
    if kind == TK_ELSE() then ret "ELSE" else 0
    if kind == TK_FOR() then ret "FOR" else 0
    if kind == TK_IN() then ret "IN" else 0
    if kind == TK_WH() then ret "WH" else 0
    if kind == TK_LP() then ret "LP" else 0
    if kind == TK_BR() then ret "BR" else 0
    if kind == TK_CT() then ret "CT" else 0
    if kind == TK_RET() then ret "RET" else 0
    if kind == TK_TRUE() then ret "TRUE" else 0
    if kind == TK_FALSE() then ret "FALSE" else 0
    if kind == TK_NONE() then ret "NONE" else 0
    if kind == TK_SOME() then ret "SOME" else 0
    if kind == TK_OK() then ret "OK" else 0
    if kind == TK_ERR() then ret "ERR" else 0
    if kind == TK_PLUS() then ret "PLUS" else 0
    if kind == TK_MINUS() then ret "MINUS" else 0
    if kind == TK_STAR() then ret "STAR" else 0
    if kind == TK_SLASH() then ret "SLASH" else 0
    if kind == TK_PERCENT() then ret "PERCENT" else 0
    if kind == TK_EQEQ() then ret "EQEQ" else 0
    if kind == TK_BANGEQ() then ret "BANGEQ" else 0
    if kind == TK_LT() then ret "LT" else 0
    if kind == TK_LTEQ() then ret "LTEQ" else 0
    if kind == TK_GT() then ret "GT" else 0
    if kind == TK_GTEQ() then ret "GTEQ" else 0
    if kind == TK_AMPAMP() then ret "AMPAMP" else 0
    if kind == TK_PIPEPIPE() then ret "PIPEPIPE" else 0
    if kind == TK_BANG() then ret "BANG" else 0
    if kind == TK_AMP() then ret "AMP" else 0
    if kind == TK_PIPE() then ret "PIPE" else 0
    if kind == TK_CARET() then ret "CARET" else 0
    if kind == TK_LTLT() then ret "LTLT" else 0
    if kind == TK_GTGT() then ret "GTGT" else 0
    if kind == TK_EQ() then ret "EQ" else 0
    if kind == TK_COLONEQ() then ret "COLONEQ" else 0
    if kind == TK_PLUSEQ() then ret "PLUSEQ" else 0
    if kind == TK_MINUSEQ() then ret "MINUSEQ" else 0
    if kind == TK_STAREQ() then ret "STAREQ" else 0
    if kind == TK_SLASHEQ() then ret "SLASHEQ" else 0
    if kind == TK_QUESTION() then ret "QUESTION" else 0
    if kind == TK_QUESTIONQUESTION() then ret "QUESTIONQUESTION" else 0
    if kind == TK_ARROW() then ret "ARROW" else 0
    if kind == TK_FATARROW() then ret "FATARROW" else 0
    if kind == TK_DOTDOT() then ret "DOTDOT" else 0
    if kind == TK_DOTDOTEQ() then ret "DOTDOTEQ" else 0
    if kind == TK_COLONCOLON() then ret "COLONCOLON" else 0
    if kind == TK_DOT() then ret "DOT" else 0
    if kind == TK_COMMA() then ret "COMMA" else 0
    if kind == TK_AT() then ret "AT" else 0
    if kind == TK_LPAREN() then ret "LPAREN" else 0
    if kind == TK_RPAREN() then ret "RPAREN" else 0
    if kind == TK_LBRACKET() then ret "LBRACKET" else 0
    if kind == TK_RBRACKET() then ret "RBRACKET" else 0
    if kind == TK_LBRACE() then ret "LBRACE" else 0
    if kind == TK_RBRACE() then ret "RBRACE" else 0
    if kind == TK_COLON() then ret "COLON" else 0
    if kind == TK_SEMICOLON() then ret "SEMICOLON" else 0
    if kind == TK_INT() then ret "INT" else 0
    if kind == TK_FLOAT() then ret "FLOAT" else 0
    if kind == TK_STRING() then ret "STRING" else 0
    if kind == TK_CHAR() then ret "CHAR" else 0
    if kind == TK_IDENT() then ret "IDENT" else 0
    if kind == TK_NEWLINE() then ret "NEWLINE" else 0
    if kind == TK_INDENT() then ret "INDENT" else 0
    if kind == TK_DEDENT() then ret "DEDENT" else 0
    if kind == TK_EOF() then ret "EOF" else 0
    if kind == TK_ERROR() then ret "ERROR" else 0
    "UNKNOWN"
# FORMA Bootstrap Compiler - Simplified Scanner
# A simpler scanner implementation that works within FORMA's syntax limitations

# Note: This file depends on token.forma being loaded first

# ============================================================
# Scanner state - simpler design using tuples instead of structs
# ============================================================

# Scanner state is represented as a tuple:
# (source: Str, pos: Int, start: Int, line: Int, column: Int, start_column: Int)

# Create new scanner state
f scanner_new(source: Str) -> (Str, Int, Int, Int, Int, Int)
    (source, 0, 0, 1, 1, 1)

# Extract parts
f scanner_source(s: (Str, Int, Int, Int, Int, Int)) -> Str
    (src, _, _, _, _, _) := s
    src

f scanner_pos(s: (Str, Int, Int, Int, Int, Int)) -> Int
    (_, pos, _, _, _, _) := s
    pos

f scanner_start(s: (Str, Int, Int, Int, Int, Int)) -> Int
    (_, _, start, _, _, _) := s
    start

f scanner_line(s: (Str, Int, Int, Int, Int, Int)) -> Int
    (_, _, _, line, _, _) := s
    line

f scanner_column(s: (Str, Int, Int, Int, Int, Int)) -> Int
    (_, _, _, _, col, _) := s
    col

f scanner_start_col(s: (Str, Int, Int, Int, Int, Int)) -> Int
    (_, _, _, _, _, start_col) := s
    start_col

# Check if at end
f at_end(s: (Str, Int, Int, Int, Int, Int)) -> Bool
    scanner_pos(s) >= str_len(scanner_source(s))

# Peek current char
f peek(s: (Str, Int, Int, Int, Int, Int)) -> Option[Char]
    if at_end(s) then None else str_char_at(scanner_source(s), scanner_pos(s))

# Peek next char
f peek_next(s: (Str, Int, Int, Int, Int, Int)) -> Option[Char]
    pos := scanner_pos(s) + 1
    if pos >= str_len(scanner_source(s)) then None else str_char_at(scanner_source(s), pos)

# Advance and return (new_state, char)
f advance(s: (Str, Int, Int, Int, Int, Int)) -> ((Str, Int, Int, Int, Int, Int), Option[Char])
    m peek(s)
        Some(c) -> ((scanner_source(s), scanner_pos(s) + 1, scanner_start(s), scanner_line(s), scanner_column(s) + 1, scanner_start_col(s)), Some(c))
        None -> (s, None)

# Match specific char
f match_char(s: (Str, Int, Int, Int, Int, Int), expected: Char) -> ((Str, Int, Int, Int, Int, Int), Bool)
    m peek(s)
        Some(c) -> if c == expected then (result, _) := advance(s); (result, true) else (s, false)
        None -> (s, false)

# Get current lexeme
f lexeme(s: (Str, Int, Int, Int, Int, Int)) -> Str
    str_slice(scanner_source(s), scanner_start(s), scanner_pos(s))

# Make span
f make_span(s: (Str, Int, Int, Int, Int, Int)) -> Span
    span_new(scanner_start(s), scanner_pos(s), scanner_line(s), scanner_start_col(s))

# Mark token start
f mark_start(s: (Str, Int, Int, Int, Int, Int)) -> (Str, Int, Int, Int, Int, Int)
    (scanner_source(s), scanner_pos(s), scanner_pos(s), scanner_line(s), scanner_column(s), scanner_column(s))

# Set at line start (for newline handling)
f newline(s: (Str, Int, Int, Int, Int, Int)) -> (Str, Int, Int, Int, Int, Int)
    (scanner_source(s), scanner_pos(s), scanner_start(s), scanner_line(s) + 1, 1, scanner_start_col(s))

# ============================================================
# Token creation
# ============================================================

f make_token(s: (Str, Int, Int, Int, Int, Int), kind: Int) -> Token
    token_new(kind, make_span(s), lexeme(s))

f make_token_int(s: (Str, Int, Int, Int, Int, Int), kind: Int, val: Int) -> Token
    token_new_int(kind, make_span(s), lexeme(s), val)

f make_token_str(s: (Str, Int, Int, Int, Int, Int), kind: Int, val: Str) -> Token
    token_new_str(kind, make_span(s), lexeme(s), val)

f make_token_char(s: (Str, Int, Int, Int, Int, Int), kind: Int, val: Char) -> Token
    token_new_char(kind, make_span(s), lexeme(s), val)

f error_token(s: (Str, Int, Int, Int, Int, Int), msg: Str) -> Token
    token_new_str(TK_ERROR(), make_span(s), lexeme(s), msg)

# ============================================================
# Skip whitespace
# ============================================================

f skip_whitespace(s: (Str, Int, Int, Int, Int, Int)) -> (Str, Int, Int, Int, Int, Int)
    done := false
    s2 := s
    wh !done
        m peek(s2)
            Some(c) -> if c == ' ' || c == '\t' || c == '\r' then (s3, _) := advance(s2); s2 = s3 else done = true
            None -> done = true
    s2

f skip_line(s: (Str, Int, Int, Int, Int, Int)) -> (Str, Int, Int, Int, Int, Int)
    done := false
    s2 := s
    wh !done
        m peek(s2)
            Some(c) -> if c == '\n' then done = true else (s3, _) := advance(s2); s2 = s3
            None -> done = true
    s2

# ============================================================
# Scan specific token types
# ============================================================

f scan_string(s: (Str, Int, Int, Int, Int, Int)) -> ((Str, Int, Int, Int, Int, Int), Token)
    value := ""
    s2 := s
    done := false
    err := ""
    wh !done
        m peek(s2)
            Some(c) ->
                if c == '"' then (s3, _) := advance(s2); s2 = s3; done = true
                else if c == '\n' then err = "unterminated string"; done = true
                else if c == '\\' then
                    (s3, _) := advance(s2)
                    m peek(s3)
                        Some(ec) ->
                            (s4, _) := advance(s3)
                            if ec == 'n' then value = str_concat(value, "\n") else if ec == 't' then value = str_concat(value, "\t") else if ec == '\\' then value = str_concat(value, "\\") else if ec == '"' then value = str_concat(value, "\"") else value = str_concat(value, char_to_str(ec))
                            s2 = s4
                        None -> err = "unterminated string"; done = true
                else value = str_concat(value, char_to_str(c)); (s3, _) := advance(s2); s2 = s3
            None -> err = "unterminated string"; done = true
    if str_len(err) > 0 then (s2, error_token(s2, err)) else (s2, make_token_str(s2, TK_STRING(), value))

f scan_char_lit(s: (Str, Int, Int, Int, Int, Int)) -> ((Str, Int, Int, Int, Int, Int), Token)
    m peek(s)
        Some(c) ->
            if c == '\'' || c == '\n' then (s, error_token(s, "empty char"))
            else if c == '\\' then
                (s2, _) := advance(s)
                m peek(s2)
                    Some(ec) ->
                        (s3, _) := advance(s2)
                        ch := if ec == 'n' then '\n' else if ec == 't' then '\t' else if ec == '\\' then '\\' else if ec == '\'' then '\'' else ec
                        (s4, matched) := match_char(s3, '\'')
                        if matched then (s4, make_token_char(s4, TK_CHAR(), ch)) else (s3, error_token(s3, "unterminated char"))
                    None -> (s2, error_token(s2, "unterminated char"))
            else
                (s2, _) := advance(s)
                (s3, matched) := match_char(s2, '\'')
                if matched then (s3, make_token_char(s3, TK_CHAR(), c)) else (s2, error_token(s2, "unterminated char"))
        None -> (s, error_token(s, "unexpected end"))

f scan_number(s: (Str, Int, Int, Int, Int, Int)) -> ((Str, Int, Int, Int, Int, Int), Token)
    s2 := s
    scanning := true
    wh scanning
        m peek(s2)
            Some(c) -> if char_is_digit(c) || c == '_' then (s3, _) := advance(s2); s2 = s3 else scanning = false
            None -> scanning = false
    lex := lexeme(s2)
    clean := str_replace_all(lex, "_", "")
    m str_to_int(clean)
        Some(n) -> (s2, make_token_int(s2, TK_INT(), n))
        None -> (s2, error_token(s2, "invalid number"))

f scan_identifier(s: (Str, Int, Int, Int, Int, Int)) -> ((Str, Int, Int, Int, Int, Int), Token)
    s2 := s
    scanning := true
    wh scanning
        m peek(s2)
            Some(c) -> if char_is_alphanumeric(c) || c == '_' then (s3, _) := advance(s2); s2 = s3 else scanning = false
            None -> scanning = false
    lex := lexeme(s2)
    m keyword_lookup(lex)
        Some(kind) -> (s2, make_token(s2, kind))
        None -> (s2, make_token_str(s2, TK_IDENT(), lex))

# ============================================================
# Main token scan
# ============================================================

f scan_token(s: (Str, Int, Int, Int, Int, Int)) -> ((Str, Int, Int, Int, Int, Int), Token)
    s2 := skip_whitespace(s)
    s2 = mark_start(s2)
    (s2, opt_c) := advance(s2)
    m opt_c
        Some(c) ->
            # Single char tokens
            if c == '(' then (s2, make_token(s2, TK_LPAREN()))
            else if c == ')' then (s2, make_token(s2, TK_RPAREN()))
            else if c == '[' then (s2, make_token(s2, TK_LBRACKET()))
            else if c == ']' then (s2, make_token(s2, TK_RBRACKET()))
            else if c == '{' then (s2, make_token(s2, TK_LBRACE()))
            else if c == '}' then (s2, make_token(s2, TK_RBRACE()))
            else if c == ',' then (s2, make_token(s2, TK_COMMA()))
            else if c == ';' then (s2, make_token(s2, TK_SEMICOLON()))
            else if c == '@' then (s2, make_token(s2, TK_AT()))
            else if c == '%' then (s2, make_token(s2, TK_PERCENT()))
            else if c == '^' then (s2, make_token(s2, TK_CARET()))
            # Two char tokens
            else if c == '+' then (s3, matched) := match_char(s2, '='); if matched then (s3, make_token(s3, TK_PLUSEQ())) else (s2, make_token(s2, TK_PLUS()))
            else if c == '-' then (s3, m1) := match_char(s2, '='); if m1 then (s3, make_token(s3, TK_MINUSEQ())) else (s4, m2) := match_char(s2, '>'); if m2 then (s4, make_token(s4, TK_ARROW())) else (s2, make_token(s2, TK_MINUS()))
            else if c == '*' then (s3, matched) := match_char(s2, '='); if matched then (s3, make_token(s3, TK_STAREQ())) else (s2, make_token(s2, TK_STAR()))
            else if c == '/' then (s3, matched) := match_char(s2, '='); if matched then (s3, make_token(s3, TK_SLASHEQ())) else (s2, make_token(s2, TK_SLASH()))
            else if c == '=' then (s3, m1) := match_char(s2, '='); if m1 then (s3, make_token(s3, TK_EQEQ())) else (s4, m2) := match_char(s2, '>'); if m2 then (s4, make_token(s4, TK_FATARROW())) else (s2, make_token(s2, TK_EQ()))
            else if c == '!' then (s3, matched) := match_char(s2, '='); if matched then (s3, make_token(s3, TK_BANGEQ())) else (s2, make_token(s2, TK_BANG()))
            else if c == '<' then (s3, m1) := match_char(s2, '='); if m1 then (s3, make_token(s3, TK_LTEQ())) else (s4, m2) := match_char(s2, '<'); if m2 then (s4, make_token(s4, TK_LTLT())) else (s2, make_token(s2, TK_LT()))
            else if c == '>' then (s3, m1) := match_char(s2, '='); if m1 then (s3, make_token(s3, TK_GTEQ())) else (s4, m2) := match_char(s2, '>'); if m2 then (s4, make_token(s4, TK_GTGT())) else (s2, make_token(s2, TK_GT()))
            else if c == '&' then (s3, matched) := match_char(s2, '&'); if matched then (s3, make_token(s3, TK_AMPAMP())) else (s2, make_token(s2, TK_AMP()))
            else if c == '|' then (s3, matched) := match_char(s2, '|'); if matched then (s3, make_token(s3, TK_PIPEPIPE())) else (s2, make_token(s2, TK_PIPE()))
            else if c == ':' then (s3, m1) := match_char(s2, '='); if m1 then (s3, make_token(s3, TK_COLONEQ())) else (s4, m2) := match_char(s2, ':'); if m2 then (s4, make_token(s4, TK_COLONCOLON())) else (s2, make_token(s2, TK_COLON()))
            else if c == '?' then (s3, matched) := match_char(s2, '?'); if matched then (s3, make_token(s3, TK_QUESTIONQUESTION())) else (s2, make_token(s2, TK_QUESTION()))
            else if c == '.' then (s3, m1) := match_char(s2, '.'); if m1 then (s4, m2) := match_char(s3, '='); if m2 then (s4, make_token(s4, TK_DOTDOTEQ())) else (s3, make_token(s3, TK_DOTDOT())) else (s2, make_token(s2, TK_DOT()))
            # Newline
            else if c == '\n' then (newline(s2), make_token(s2, TK_NEWLINE()))
            # Comment
            else if c == '#' then s3 := skip_line(s2); scan_token(s3)
            # String
            else if c == '"' then scan_string(s2)
            # Char
            else if c == '\'' then scan_char_lit(s2)
            # Number
            else if char_is_digit(c) then scan_number(s2)
            # Identifier
            else if char_is_alpha(c) || c == '_' then scan_identifier(s2)
            # Unknown
            else (s2, error_token(s2, str_concat("unexpected char: ", char_to_str(c))))
        None -> (s2, make_token(s2, TK_EOF()))

# ============================================================
# Scan all tokens
# ============================================================

f scan_all_tokens(source: Str) -> [Token]
    s := scanner_new(source)
    tokens := []
    done := false
    wh !done
        (s2, tok) := scan_token(s)
        s = s2
        tokens = vec_push(tokens, tok)
        if tok.kind == TK_EOF() then done = true else done = false
    tokens
# Simple lexer test
# Expected output: 1

f test_operators() -> Bool
    tokens := scan_all_tokens("+ - * /")
    vec_len(tokens) >= 5 && tokens[0].kind == TK_PLUS() && tokens[1].kind == TK_MINUS()

f test_keywords() -> Bool
    tokens := scan_all_tokens("f if then else")
    vec_len(tokens) >= 4 && tokens[0].kind == TK_F() && tokens[1].kind == TK_IF()

f test_number() -> Bool
    tokens := scan_all_tokens("42")
    vec_len(tokens) >= 1 && tokens[0].kind == TK_INT() && tokens[0].int_val == 42

f test_string() -> Bool
    tokens := scan_all_tokens("\"hello\"")
    vec_len(tokens) >= 1 && tokens[0].kind == TK_STRING() && tokens[0].str_val == "hello"

f test_identifier() -> Bool
    tokens := scan_all_tokens("foo")
    vec_len(tokens) >= 1 && tokens[0].kind == TK_IDENT() && tokens[0].str_val == "foo"

f run_all() -> Int
    passed := 0
    if test_operators() then passed = passed + 1 else print("FAIL: operators")
    if test_keywords() then passed = passed + 1 else print("FAIL: keywords")
    if test_number() then passed = passed + 1 else print("FAIL: number")
    if test_string() then passed = passed + 1 else print("FAIL: string")
    if test_identifier() then passed = passed + 1 else print("FAIL: identifier")
    print("Lexer tests passed:")
    print(passed)
    print("of 5")
    if passed == 5 then 1 else 0

f main() -> Int = run_all()
